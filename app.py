

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder

# Set plotting style
sns.set_style("whitegrid")

# --- 1. Load Data, Display Info, and Shape ---
print("--- 1. Import and Load Data ---")
file_path = 'California_Historic_Fire_Perimeters_-6236829869961296710.csv'
try:
    df = pd.read_csv(file_path, low_memory=False)
except FileNotFoundError:
    print(f"Error: File not found at {file_path}.")
    raise

print("\n--- DataFrame Info ---")
df.info(verbose=False, memory_usage="deep")
print("\n--- DataFrame Shape ---")
print(df.shape)
print("\n--- DataFrame Head (First 5 Rows) ---")
print(df.head())


# --- 2. Data Cleaning and Feature Engineering ---
print("\n--- 2. Data Cleaning and Feature Engineering ---")

# Define key columns for reference
TARGET_ACRES = 'GIS Calculated Acres'
TARGET_CAUSE = 'Cause'
DATE_COLS = ['Alarm Date', 'Containment Date']

# 2.1 Convert Date Columns and Create Duration Feature
for col in DATE_COLS:
    # Coerce errors to NaT (Not a Time) for dates that cannot be parsed
    df[col] = pd.to_datetime(df[col], errors='coerce')

# Calculate fire duration in days
df['Fire_Duration_Days'] = (df['Containment Date'] - df['Alarm Date']).dt.total_seconds() / (60*60*24)
print(f"Created feature 'Fire_Duration_Days'. Missing values: {df['Fire_Duration_Days'].isnull().sum()}")

# 2.2 Handle Missing Values and Illogical Data
# Drop rows where the target (Acres) is missing or duration is negative/missing
df.dropna(subset=[TARGET_ACRES], inplace=True)
df = df[df[TARGET_ACRES] > 0]       # Exclude fires with zero acres
df = df[df['Fire_Duration_Days'] >= 0] # Exclude illogical negative durations

# Impute 'Cause' NaNs with 'Unknown'
df[TARGET_CAUSE].fillna('Unknown', inplace=True)
print(f"Cleaned data shape after dropping NaNs/zero-acre fires: {df.shape}")


# 2.3 Create Classification Target: 'Large_Fire'
# Large fire is defined as top 25% of acres
ACRE_THRESHOLD = df[TARGET_ACRES].quantile(0.75)
print(f"Using {ACRE_THRESHOLD:.2f} acres as the 75th percentile threshold for 'Large_Fire'.")
df['Large_Fire'] = (df[TARGET_ACRES] > ACRE_THRESHOLD).astype(int)

# 2.4 Create Simplified Cause Categories for Classification (Top 5 causes + Other)
cause_counts = df[TARGET_CAUSE].value_counts()
top_n = 5
top_causes = cause_counts.head(top_n).index.tolist()
df['Cause_Simplified'] = df[TARGET_CAUSE].apply(lambda x: str(x) if x in top_causes else 'Other')
print(f"Top {top_n} causes: {top_causes}. Created 'Cause_Simplified'.")


# --- 3. Exploratory Data Analysis (EDA) ---
print("\n--- 3. Exploratory Data Analysis (EDA) ---")

# 3.1 Descriptive Statistics
key_numeric_cols = [TARGET_ACRES, 'Fire_Duration_Days', 'Year']
print("\n--- Descriptive Statistics (Acres, Duration, Year) ---")
print(df[key_numeric_cols].describe().T)

# 3.2 Univariate: Distribution of Acres (Target Variable)
plt.figure(figsize=(10, 6))
# Using log-transformation for visualization due to extreme skewness
sns.histplot(df[TARGET_ACRES].apply(np.log1p), kde=True, bins=50)
plt.title(f'Distribution of Log-transformed {TARGET_ACRES}')
plt.xlabel(f'Log({TARGET_ACRES} + 1)')
plt.savefig('acres_distribution_log.png')
plt.close()
#  is generated by the execution

# 3.3 Bivariate: Acres vs Duration
plt.figure(figsize=(10, 6))
# Log-transform both for scatterplot
sns.scatterplot(x=df['Fire_Duration_Days'].apply(np.log1p), y=df[TARGET_ACRES].apply(np.log1p), alpha=0.5)
plt.title('Log(Acres) vs. Log(Duration)')
plt.xlabel('Log(Fire_Duration_Days + 1)')
plt.ylabel(f'Log({TARGET_ACRES} + 1)')
plt.savefig('acres_vs_duration.png')
plt.close()
#  is generated by the execution

# 3.4 Categorical: Count Plot for Simplified Cause
plt.figure(figsize=(12, 6))
sns.countplot(y='Cause_Simplified', data=df, order=df['Cause_Simplified'].value_counts().index, palette='coolwarm')
plt.title('Count of Fires by Simplified Cause')
plt.savefig('cause_simplified_counts.png')
plt.close()
#  is generated by the execution

# Save the cleaned DataFrame for the next steps
df.to_csv('df_fire_cleaned.csv', index=False)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
import numpy as np
import warnings

warnings.filterwarnings('ignore', category=FutureWarning)

# --- 1. Load Cleaned Data ---
try:
    df = pd.read_csv('df_fire_cleaned.csv')
except FileNotFoundError:
    print("ERROR: df_fire_cleaned.csv not found. Please ensure the EDA/Cleaning step was run successfully.")
    raise

# --- 2. Define Features and Targets ---

# Regression Target: Log-transform acres
REGRESSION_TARGET = 'GIS Calculated Acres_log'
df[REGRESSION_TARGET] = np.log1p(df['GIS Calculated Acres'])

# Classification Target
CLASSIFICATION_TARGET = 'Large_Fire'

# Features to EXCLUDE (ID/Text/Original targets)
EXCLUDE_COLS = [
    'OBJECTID', 'Fire Name', 'Local Incident Number', 'Alarm Date', 'Containment Date',
    'Comments', 'Complex Name', 'IRWIN ID', 'Fire Number (historical use)', 'Complex ID',
    'DECADES', 'GIS Calculated Acres', 'Cause', 'State', 'Agency', 'Unit ID'
]

# Define Feature Matrix X
X = df.drop(columns=[REGRESSION_TARGET, CLASSIFICATION_TARGET] + EXCLUDE_COLS, errors='ignore')

# Identify final numeric and categorical features
numeric_features = X.select_dtypes(include=np.number).columns.tolist()
categorical_features = ['Cause_Simplified'] # Simplified cause is used

# --- 3. Preprocessing Pipeline (Scaling and Encoding) ---

# Define preprocessing steps using ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        # 1. Scaling for numeric features
        ('num', StandardScaler(), numeric_features),
        # 2. One-Hot Encoding for the simplified cause
        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)
    ],
    remainder='drop'
)

# Fit and transform the features
X_processed = preprocessor.fit_transform(X)

# Get the final feature names and create a DataFrame
feature_names = numeric_features + list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features))
X_final_df = pd.DataFrame(X_processed, columns=feature_names)

# Fill any residual NaNs with 0 (as a final safeguard before ML models)
X_final_df.fillna(0, inplace=True)


print("\n--- Final Preprocessed Data Snapshot (First 5 Rows) ---")
print(X_final_df.head())
print(f"\nFinal Feature Matrix Shape: {X_final_df.shape}")


# --- 4. Split Datasets (Train/Test: 80/20) ---

# Target for Regression
y_reg = df[REGRESSION_TARGET]

# Target for Classification
y_cls = df[CLASSIFICATION_TARGET]

# Split the data
X_train, X_test, y_reg_train, y_reg_test, y_cls_train, y_cls_test = train_test_split(
    X_final_df, y_reg, y_cls, test_size=0.2, random_state=42
)

print("\n--- Data Split Summary ---")
print(f"X_train shape: {X_train.shape}")
print(f"y_reg_train shape (Regression Target): {y_reg_train.shape}")
print(f"y_cls_train shape (Classification Target): {y_cls_train.shape}")

# Save the split data for the subsequent ML/Clustering tasks
X_train.to_csv('X_train.csv', index=False)
X_test.to_csv('X_test.csv', index=False)
y_reg_train.to_csv('y_reg_train.csv', index=False)
y_reg_test.to_csv('y_reg_test.csv', index=False)
y_cls_train.to_csv('y_cls_train.csv', index=False)
y_cls_test.to_csv('y_cls_test.csv', index=False)
print("\nSplit training and testing data saved to CSV files.")

import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
    confusion_matrix, classification_report
)
import warnings

warnings.filterwarnings('ignore')

# --- 1. Load Data ---
print("--- 1. Loading Preprocessed Classification Data ---")
try:
    X_train = pd.read_csv('X_train.csv')
    X_test = pd.read_csv('X_test.csv')
    # Target is the binary 'Large_Fire' column
    y_train = pd.read_csv('y_cls_train.csv').squeeze()
    y_test = pd.read_csv('y_cls_test.csv').squeeze()
except FileNotFoundError:
    print("ERROR: Classification data files not found. Please ensure the Preprocessing step was run successfully.")
    raise

# Final check and imputation of any remaining NaNs (imputed with 0 in Preprocessing, but as a safeguard)
X_train.fillna(0, inplace=True)
X_test.fillna(0, inplace=True)


# --- 2. Define Metrics Function ---
def evaluate_classification_model(y_true, y_pred, y_pred_proba, model_name):
    """Calculates and returns standard classification metrics."""

    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    # ROC AUC requires probabilities for the positive class (column 1)
    auc = roc_auc_score(y_true, y_pred_proba[:, 1])

    print(f"\nModel: {model_name}")
    print(f"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}, AUC: {auc:.4f}")

    return {
        'Model': name,
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1-Score': f1,
        'AUC': auc
    }


# --- 3. Train and Evaluate Models ---
models = {
    "Logistic Regression": LogisticRegression(random_state=42, solver='liblinear'),
    "Decision Tree Classifier": DecisionTreeClassifier(random_state=42),
    "Random Forest Classifier": RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),
}

classification_results = []

for name, model in models.items():
    print(f"\n--- Training {name} ---")

    # Fit training data
    model.fit(X_train, y_train)

    # Predict on test data
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test) # Needed for AUC

    # Evaluate metrics and store results
    result = evaluate_classification_model(y_test, y_pred, y_pred_proba, name)
    classification_results.append(result)

# --- 4. Display Metrics and Comparison Table ---
comparison_df = pd.DataFrame(classification_results).set_index('Model')
comparison_df.sort_values(by='F1-Score', ascending=False, inplace=True)

print("\n\n--- Classification Model Comparison Table (Target: Large_Fire) ---")
print(comparison_df)

# Save the comparison table
comparison_df.to_csv('classification_model_comparison.csv')
print("\nClassification comparison results saved to 'classification_model_comparison.csv'.")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
import warnings

warnings.filterwarnings('ignore')

# --- 1. Load Full Feature Data ---
print("--- 1. Loading and Preparing Full Feature Data for Clustering ---")
try:
    X_train = pd.read_csv('X_train.csv')
    X_test = pd.read_csv('X_test.csv')
    # Combine training and testing data for a single clustering analysis
    X_full = pd.concat([X_train, X_test], ignore_index=True)
except FileNotFoundError:
    print("ERROR: Feature data files not found. Please ensure the Preprocessing step was run successfully.")
    raise

# Final check and imputation of any remaining NaNs
X_full.fillna(0, inplace=True)
print(f"Full Feature Matrix Shape: {X_full.shape}")

# --- 2. Determine Optimal K (Elbow Method) ---
print("\n--- 2. Determining Optimal K using Elbow Method ---")
# Test K from 1 to 10
K_RANGE = range(1, 11)
inertia = []

# Using a subset of the data for faster computation of Elbow Method
X_sample = X_full.sample(min(10000, len(X_full)), random_state=42)

for k in K_RANGE:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10, max_iter=300)
    kmeans.fit(X_sample)
    inertia.append(kmeans.inertia_)

# Plot Elbow Curve
plt.figure(figsize=(8, 5))
plt.plot(K_RANGE, inertia, marker='o', linestyle='--', color='blue')
plt.title('Elbow Method for Optimal K')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('Inertia (Sum of Squared Distances)')
plt.xticks(K_RANGE)
plt.savefig('clustering_elbow_method.png')
plt.close()
print(" (Elbow Method)")


# --- 3. Apply K-Means Clustering (Select K=3 or K=4 based on typical elbow observation) ---
# Assuming K=3 is a reasonable elbow point for demonstration purposes
OPTIMAL_K = 3
print(f"\n--- 3. Applying K-Means Clustering with K={OPTIMAL_K} ---")

kmeans_final = KMeans(n_clusters=OPTIMAL_K, random_state=42, n_init=10)
X_full['Cluster'] = kmeans_final.fit_predict(X_full)

print(f"Cluster sizes:\n{X_full['Cluster'].value_counts()}")

# --- 4. Evaluate Clustering (Silhouette Score) ---
# Note: Silhouette score can be slow on large datasets; using a subset for speed.
if len(X_full) > 10000:
    # Use a smaller sample for the score calculation
    X_score_sample = X_full.sample(5000, random_state=42).drop(columns=['Cluster'])
    labels_sample = X_full.loc[X_score_sample.index, 'Cluster']
    score = silhouette_score(X_score_sample, labels_sample)
else:
    score = silhouette_score(X_full.drop(columns=['Cluster']), X_full['Cluster'])

print(f"\nSilhouette Score (Measures cluster separation): {score:.4f}")


# --- 5. Visualize Clusters using PCA ---
print("\n--- 5. Visualizing Clusters with PCA ---")

# Reduce the feature space to 2 dimensions
pca = PCA(n_components=2, random_state=42)
components = pca.fit_transform(X_full.drop(columns=['Cluster']))

pca_df = pd.DataFrame(data=components, columns=['PCA_Component_1', 'PCA_Component_2'])
pca_df['Cluster'] = X_full['Cluster']

plt.figure(figsize=(10, 8))
sns.scatterplot(
    x='PCA_Component_1',
    y='PCA_Component_2',
    hue='Cluster',
    data=pca_df,
    palette='viridis',
    legend='full',
    alpha=0.6
)
plt.title(f'K-Means Clustering (K={OPTIMAL_K}) Visualization via PCA')
plt.xlabel(f'PCA Component 1 ({pca.explained_variance_ratio_[0]*100:.2f}% Variance)')
plt.ylabel(f'PCA Component 2 ({pca.explained_variance_ratio_[1]*100:.2f}% Variance)')
plt.savefig('clustering_pca_visualization.png')
plt.close()
print(" (PCA Plot of Clusters)")

import pandas as pd
from statsmodels.tsa.arima.model import ARIMA
import numpy as np

# Define the file name (replace with your actual file name if it changes)
file_name = "/content/drive/MyDrive/California_Historic_Dataset/California_Historic_Fire_Perimeters_-6236829869961296710.csv"

# --- 1. Data Aggregation to create the time series ---
df = pd.read_csv(file_name)
df_ts = df[['Year', 'GIS Calculated Acres']].dropna(subset=['Year'])
df_ts['Year'] = df_ts['Year'].astype(int)

# Group by year and sum the acres
ts_data = df_ts.groupby('Year')['GIS Calculated Acres'].sum().reset_index()

# Set 'Year' as a datetime index
ts_data['Year'] = pd.to_datetime(ts_data['Year'], format='%Y')
ts_data.set_index('Year', inplace=True)

# Filter out incomplete future data (e.g., Year 2025)
ts_data = ts_data[ts_data.index.year < 2025]

# --- 2. Data Preparation for ARIMA ---
# Resample to ensure a continuous series (fills missing years between the min and max year)
ts_full = ts_data.resample('Y').sum().fillna(0)
series = ts_full['GIS Calculated Acres']

# 3. Split the data (using the last 10 years for testing/evaluation)
train_series = series.iloc[:-10]
test_series = series.iloc[-10:]

# 4. Fit the ARIMA model
# A simple ARIMA(5, 1, 0) order is chosen for demonstration:
# p=5 (Auto-Regressive), d=1 (Integrated/Differencing), q=0 (Moving Average)
try:
    print("Fitting ARIMA model...")
    arima_model = ARIMA(train_series, order=(5, 1, 0)).fit()
    print("ARIMA model fit complete.")

    # 5. Make predictions for the test period (last 10 years)
    forecast = arima_model.get_forecast(steps=len(test_series))
    forecast_mean = forecast.predicted_mean

    # 6. Evaluate the model
    rmse = np.sqrt(np.mean((test_series.values - forecast_mean.values)**2))

    print("\n--- ARIMA Time Series Analysis Results ---")
    print(f"Root Mean Squared Error (RMSE): {rmse:,.2f} Acres")

    # Display Actual vs. Predicted values
    results = pd.DataFrame({
        'Actual Acres': test_series.values.round(2),
        'Predicted Acres': forecast_mean.values.round(2)
    }, index=test_series.index.year)

    print("\nActual vs. Predicted (Test Set):")
    print(results.to_markdown(numalign="left", stralign="left"))

except Exception as e:
    print(f"An error occurred during ARIMA modeling (Convergence Warning is common, but can prevent a full run): {e}")
